# üöÄ Setup Multi-GPU para Treinamento Dynamic GNN

**Data:** 06 de Novembro de 2025  
**GPUs Dispon√≠veis:**
- Node atual: NVIDIA RTX 4000 Ada Generation (19.2 GB)
- Node maria: NVIDIA L4 24GB

---

## üìä OP√á√ïES DE CONFIGURA√á√ÉO

### Op√ß√£o 1: Treinamento Separado (Recomendado)

Rodar treinamento em cada node separadamente:

#### Node Atual (RTX 4000):
```bash
# J√° est√° rodando
python3 scripts/train_dynamic_gnn_pbpk.py \
    --data data/dynamic_gnn_training_full/training_data.npz \
    --output models/dynamic_gnn_full \
    --epochs 50 \
    --batch-size 16 \
    --lr 1e-3 \
    --device cuda
```

#### Node Maria (L4 24GB):
```bash
# SSH para node maria
ssh maria

# Rodar treinamento
cd ~/workspace/darwin-pbpk-platform
./scripts/run_training_maria.sh

# Ou manualmente:
python3 scripts/train_dynamic_gnn_pbpk.py \
    --data data/dynamic_gnn_training_full/training_data.npz \
    --output models/dynamic_gnn_maria \
    --epochs 50 \
    --batch-size 32 \  # L4 tem mais mem√≥ria, pode usar batch maior
    --lr 1e-3 \
    --device cuda
```

**Vantagens:**
- ‚úÖ Simples de configurar
- ‚úÖ Cada GPU trabalha independentemente
- ‚úÖ Pode usar batch sizes diferentes (L4 tem mais mem√≥ria)
- ‚úÖ F√°cil de monitorar

**Desvantagens:**
- ‚ö†Ô∏è N√£o sincroniza gradientes (mas OK para este caso)

---

### Op√ß√£o 2: DistributedDataParallel (DDP) Multi-Node

Usar ambas as GPUs com sincroniza√ß√£o de gradientes:

#### Setup:

1. **No node master (atual):**
```bash
# Iniciar processo master
python3 scripts/train_dynamic_gnn_ddp.py \
    --data data/dynamic_gnn_training_full/training_data.npz \
    --output models/dynamic_gnn_ddp \
    --epochs 50 \
    --batch-size 16 \
    --lr 1e-3 \
    --world-size 2 \
    --master-addr $(hostname -I | awk '{print $1}') \
    --master-port 12355
```

2. **No node maria:**
```bash
# SSH e rodar worker
ssh maria
cd ~/workspace/darwin-pbpk-platform

# Rodar worker (mesmo comando, mas precisa do master_addr)
python3 scripts/train_dynamic_gnn_ddp.py \
    --data data/dynamic_gnn_training_full/training_data.npz \
    --output models/dynamic_gnn_ddp \
    --epochs 50 \
    --batch-size 16 \
    --lr 1e-3 \
    --world-size 2 \
    --master-addr <IP_DO_NODE_MASTER> \
    --master-port 12355
```

**Vantagens:**
- ‚úÖ Sincroniza gradientes entre nodes
- ‚úÖ Treinamento mais consistente
- ‚úÖ Melhor para datasets muito grandes

**Desvantagens:**
- ‚ö†Ô∏è Mais complexo de configurar
- ‚ö†Ô∏è Requer comunica√ß√£o de rede entre nodes
- ‚ö†Ô∏è Pode ser mais lento devido a overhead de comunica√ß√£o

---

## üéØ RECOMENDA√á√ÉO

**Para este caso, recomendo Op√ß√£o 1 (Treinamento Separado):**

1. **Node atual (RTX 4000):** J√° est√° rodando, deixar continuar
2. **Node maria (L4 24GB):** Rodar treinamento separado com batch_size maior

**Raz√µes:**
- Dataset n√£o √© t√£o grande (1000 amostras)
- Cada GPU pode completar treinamento independentemente
- L4 tem mais mem√≥ria (24GB vs 19.2GB), pode usar batch_size 32
- Mais simples e menos propenso a erros

---

## üìä COMPARA√á√ÉO DE PERFORMANCE

### RTX 4000 Ada (Node Atual):
- **Mem√≥ria:** 19.2 GB
- **Batch size:** 16
- **Tempo/√©poca:** ~14-15 min
- **Tempo total:** ~12-13 horas

### L4 24GB (Node Maria):
- **Mem√≥ria:** 24 GB
- **Batch size:** 32 (recomendado, 2x mais r√°pido)
- **Tempo/√©poca estimado:** ~7-8 min
- **Tempo total estimado:** ~6-7 horas

**Com ambas rodando:**
- Voc√™ ter√° 2 modelos treinados
- Pode comparar resultados
- Ou usar ensemble dos 2 modelos

---

## üöÄ QUICK START - Node Maria

```bash
# 1. SSH para node maria
ssh maria

# 2. Ir para diret√≥rio do projeto
cd ~/workspace/darwin-pbpk-platform

# 3. Rodar script
./scripts/run_training_maria.sh

# 4. Monitorar
tail -f training_maria.log
```

---

## üìÅ ARQUIVOS

- `scripts/train_dynamic_gnn_ddp.py` - Script DDP (multi-node)
- `scripts/run_training_maria.sh` - Script para node maria
- `models/dynamic_gnn_full/` - Modelo do node atual
- `models/dynamic_gnn_maria/` - Modelo do node maria (ser√° criado)

---

**Status:** ‚úÖ Scripts prontos para usar ambas as GPUs

